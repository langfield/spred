{
  "afn": "gelu",
  "attn_pdrop": 0.1,
  "embd_pdrop": 0.1,
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "n_ctx": 60,
  "n_embd": 60,
  "n_head": 5,
  "n_layer": 8,
  "n_positions": 120,
  "num_labels": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "predict_special_tokens": true,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "token_ids",
  "summary_use_proj": true,
  "torchscript": false,
  "vocab_size": 1
}
