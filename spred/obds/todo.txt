Done:   #================================DONE==================================

        Finished generating preliminary graphs.

        Moved best delta proportion calculations into their own function.

        Generated per-level statistics, gap-frequency table, and three-
        sigma confidence intervals for the randon variable Y representing
        the future best price on both sides.

        Cleaned up the function structure in ``format.py``.

        Compute something involving a sequence of all gaps on each side.
        We currently compute a distribution of all nonzero volume level
        gaps on both sides of the orderbook and generate a histogram for
        both on the same plot. This means that we see gaps from all depths
        of the orderbook at all timesteps all on the same plot. This is
        a start, but we don't really care about really deep gaps. We only
        care about maybe the first 5-10 or so. More importantly, it is
        important that we can see the distribution of gaps for just the
        best nonzero level to the second best nonzero level. This will
        tell us valuable information about the number of zero-volume
        levels we need to compute the conditional probability of using
        Sirignano's model.

        Create a ``figs/`` directory.

        Move hardcoded arguments from the bottom of ``format.py`` into
        a bash script with arguments.

        Add argparser to ``format.py``.

        Merge ``obds`` to master.

        Create new dev branch.

Todo:   #================================TODO==================================

        Create a script to convert raw orderbook hour json files into
        monolithic csv files with dimension 300.

        Use a multiprocessing Pool. Get a specific start time, then generate
        an array of parse times for each of the processes. Then use python's
        built-in ``sched`` library to scrape at exactly those times in each
        process, where each one has its own Tor-assigned IP address. Then we
        can collect all of the resuls of the parses as dict key-val pairs
        where the key is the token parse time in the iterable that was passe to
        the Pool worker. Then the return iterable can be sorted as the keys
        come in.
